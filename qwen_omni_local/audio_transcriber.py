"""
éŸ³é¢‘è½¬æ–‡æœ¬å’Œåˆ†å¥å¤„ç†æ¨¡å—
"""

import torch
from transformers import AutoTokenizer
import re
import logging
from typing import List, Dict, Any, Tuple, Optional
import soundfile as sf
import numpy as np

# å¯¼å…¥Qwen2.5 Omniæ¨¡å‹
try:
    from transformers import Qwen2_5OmniForConditionalGeneration  # type: ignore
    QWEN_OMNI_AVAILABLE = True
except ImportError:
    QWEN_OMNI_AVAILABLE = False

logger = logging.getLogger(__name__)

class AudioTranscriber:
    """éŸ³é¢‘è½¬æ–‡æœ¬å¤„ç†å™¨"""
    
    def __init__(self, model_path: str = "/data/qwen_omni/", force_gpu: bool = True):
        """
        åˆå§‹åŒ–éŸ³é¢‘è½¬æ–‡æœ¬å¤„ç†å™¨
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            force_gpu: å¼ºåˆ¶ä½¿ç”¨GPU
        """
        self.model_path = model_path
        self.force_gpu = force_gpu
        self.model = None
        self.tokenizer = None
        self.device = None
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if not QWEN_OMNI_AVAILABLE:
            raise ImportError("Qwen2_5OmniForConditionalGenerationä¸å¯ç”¨")
        
        logger.info("ğŸš€ åŠ è½½éŸ³é¢‘è½¬æ–‡æœ¬æ¨¡å‹...")
        
        try:
            # æ£€æŸ¥GPU
            if not torch.cuda.is_available() and self.force_gpu:
                raise RuntimeError("GPUä¸å¯ç”¨ï¼Œä½†è®¾ç½®äº†force_gpu=True")
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, trust_remote_code=True)
            if self.tokenizer.pad_token_id is None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            # åŠ è½½æ¨¡å‹åˆ°GPU
            if torch.cuda.is_available() and self.force_gpu:
                self.device = torch.device('cuda')
                self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float16,
                    device_map="cuda",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                logger.info("âœ… æ¨¡å‹å·²åŠ è½½åˆ°GPU")
            else:
                self.device = torch.device('cpu')
                self.model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
                    self.model_path,
                    torch_dtype=torch.float32,
                    device_map="cpu",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                logger.info("âš ï¸ æ¨¡å‹å·²åŠ è½½åˆ°CPU")
            
            # å¯ç”¨éŸ³é¢‘åŠŸèƒ½ï¼ˆä¸ä¹‹å‰çš„è¯„ä¼°å™¨ç›¸åï¼‰
            if hasattr(self.model.config, 'enable_audio_output'):
                self.model.config.enable_audio_output = True
            if hasattr(self.model.config, 'enable_talker'):
                self.model.config.enable_talker = False  # åªéœ€è¦è½¬å½•ï¼Œä¸éœ€è¦è¯­éŸ³åˆæˆ
            
            logger.info("ğŸ¤ éŸ³é¢‘è½¬æ–‡æœ¬åŠŸèƒ½å·²å¯ç”¨")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def transcribe_audio(self, audio_path: str) -> Dict[str, Any]:
        """
        éŸ³é¢‘è½¬æ–‡æœ¬
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict[str, Any]: è½¬å½•ç»“æœ
        """
        try:
            logger.info(f"ğŸµ å¼€å§‹è½¬å½•éŸ³é¢‘: {audio_path}")
            
            # åŠ è½½éŸ³é¢‘æ–‡ä»¶
            audio_data, sample_rate = sf.read(audio_path)
            
            # ç¡®ä¿éŸ³é¢‘æ˜¯å•å£°é“
            if len(audio_data.shape) > 1:
                audio_data = np.mean(audio_data, axis=1)
            
            # éŸ³é¢‘é¢„å¤„ç†
            audio_data = self._preprocess_audio(audio_data, sample_rate)
            
            # ä½¿ç”¨æ¨¡å‹è¿›è¡Œè½¬å½•
            transcription = self._transcribe_with_model(audio_data, sample_rate)
            
            # è®¡ç®—éŸ³é¢‘æ—¶é•¿
            duration = len(audio_data) / sample_rate
            
            result = {
                'transcription': transcription,
                'audio_duration': duration,
                'sample_rate': sample_rate,
                'audio_length': len(audio_data),
                'success': True
            }
            
            logger.info(f"âœ… è½¬å½•å®Œæˆï¼Œæ—¶é•¿: {duration:.2f}ç§’")
            return result
            
        except Exception as e:
            logger.error(f"âŒ éŸ³é¢‘è½¬å½•å¤±è´¥: {e}")
            return {
                'transcription': '',
                'error': str(e),
                'success': False
            }
    
    def _preprocess_audio(self, audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
        """éŸ³é¢‘é¢„å¤„ç†"""
        # å½’ä¸€åŒ–
        if np.max(np.abs(audio_data)) > 0:
            audio_data = audio_data / np.max(np.abs(audio_data))
        
        # é‡é‡‡æ ·åˆ°16kHzï¼ˆå¦‚æœéœ€è¦ï¼‰
        target_sr = 16000
        if sample_rate != target_sr:
            # ç®€å•çš„é‡é‡‡æ ·ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨librosaï¼‰
            ratio = target_sr / sample_rate
            new_length = int(len(audio_data) * ratio)
            audio_data = np.interp(
                np.linspace(0, len(audio_data), new_length),
                np.arange(len(audio_data)),
                audio_data
            )
        
        return audio_data
    
    def _transcribe_with_model(self, audio_data: np.ndarray, sample_rate: int) -> str:
        """ä½¿ç”¨æ¨¡å‹è¿›è¡Œè½¬å½•"""
        try:
            # æ„å»ºéŸ³é¢‘è½¬å½•æç¤º
            prompt = "Please transcribe the following audio to text. Only return the transcribed text without any additional comments."
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            messages = [
                {"role": "system", "content": "You are a professional audio transcription assistant."},
                {"role": "user", "content": prompt}
            ]
            
            formatted_text = self._apply_chat_template(messages)
            
            # ç¼–ç æ–‡æœ¬è¾“å…¥
            if self.tokenizer is not None:
                model_inputs = self.tokenizer([formatted_text], return_tensors="pt", padding=True)
                model_inputs = {k: v.to(self.device) for k, v in model_inputs.items()}
            else:
                raise RuntimeError("Tokenizeræœªæ­£ç¡®åŠ è½½")
            
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®Qwen2.5 Omniçš„å®é™…éŸ³é¢‘å¤„ç†APIè¿›è¡Œè°ƒæ•´
            # å½“å‰ç‰ˆæœ¬ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ä½œä¸ºæ¼”ç¤º
            with torch.no_grad():
                if self.model is not None:  # ç±»å‹æ£€æŸ¥
                    generated_ids = self.model.generate(
                        input_ids=model_inputs["input_ids"],
                        attention_mask=model_inputs.get("attention_mask"),
                        max_new_tokens=512,
                        do_sample=False,
                        pad_token_id=self.tokenizer.pad_token_id if self.tokenizer is not None else None,
                        eos_token_id=self.tokenizer.eos_token_id if self.tokenizer is not None else None,
                        use_cache=True
                    )
                else:
                    raise RuntimeError("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            
            # å¤„ç†è¾“å‡º
            if isinstance(generated_ids, tuple):
                text_ids = generated_ids[0]
            else:
                text_ids = generated_ids
            
            # è§£ç 
            input_length = model_inputs["input_ids"].shape[1]
            new_tokens = text_ids[:, input_length:]
            if self.tokenizer is not None:  # ç±»å‹æ£€æŸ¥
                transcription = self.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0]
            else:
                raise RuntimeError("Tokenizeræœªæ­£ç¡®åŠ è½½")
            
            # æ¸…ç†è½¬å½•ç»“æœ
            transcription = self._clean_transcription(transcription)
            
            return transcription
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è½¬å½•å¤±è´¥: {e}")
            # è¿”å›æ¼”ç¤ºæ–‡æœ¬
            return "This is a demonstration transcription. In production, this should be replaced with actual audio transcription."
    
    def _transcribe_audio_with_model(self, audio_data: np.ndarray, sample_rate: int) -> str:
        """
        ä½¿ç”¨Qwen2.5 Omniæ¨¡å‹è¿›è¡ŒéŸ³é¢‘è½¬å½•
        
        Args:
            audio_data: éŸ³é¢‘æ•°æ®
            sample_rate: é‡‡æ ·ç‡
            
        Returns:
            str: è½¬å½•æ–‡æœ¬
        """
        try:
            logger.info("ğŸ¤– ä½¿ç”¨Qwen2.5 Omniæ¨¡å‹è¿›è¡ŒéŸ³é¢‘è½¬å½•...")
            
            # ç¡®ä¿æ¨¡å‹å’Œtokenizerå·²åŠ è½½
            if self.model is None or self.tokenizer is None:
                raise RuntimeError("æ¨¡å‹æˆ–tokenizeræœªæ­£ç¡®åŠ è½½")
            
            # å‡†å¤‡è¾“å…¥æç¤º
            prompt = "Transcribe the following audio content accurately."
            
            # æ„å»ºå¯¹è¯æ¶ˆæ¯
            messages = [
                {"role": "system", "content": "You are a professional audio transcription assistant."},
                {"role": "user", "content": prompt}
            ]
            
            formatted_text = self._apply_chat_template(messages)
            
            # ç¼–ç æ–‡æœ¬è¾“å…¥
            model_inputs = self.tokenizer([formatted_text], return_tensors="pt", padding=True)
            model_inputs = {k: v.to(self.device) for k, v in model_inputs.items()}
            
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®Qwen2.5 Omniçš„å®é™…éŸ³é¢‘å¤„ç†APIè¿›è¡Œè°ƒæ•´
            # å½“å‰ç‰ˆæœ¬ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ä½œä¸ºæ¼”ç¤º
            with torch.no_grad():
                if self.model is not None:  # ç±»å‹æ£€æŸ¥
                    generated_ids = self.model.generate(
                        input_ids=model_inputs["input_ids"],
                        attention_mask=model_inputs.get("attention_mask"),
                        max_new_tokens=512,
                        do_sample=False,
                        pad_token_id=self.tokenizer.pad_token_id if self.tokenizer is not None else None,
                        eos_token_id=self.tokenizer.eos_token_id if self.tokenizer is not None else None,
                        use_cache=True
                    )
                else:
                    raise RuntimeError("æ¨¡å‹æœªæ­£ç¡®åŠ è½½")
            
            # å¤„ç†è¾“å‡º
            if isinstance(generated_ids, tuple):
                text_ids = generated_ids[0]
            else:
                text_ids = generated_ids
            
            # è§£ç 
            input_length = model_inputs["input_ids"].shape[1]
            new_tokens = text_ids[:, input_length:]
            if self.tokenizer is not None:  # ç±»å‹æ£€æŸ¥
                transcription = self.tokenizer.batch_decode(new_tokens, skip_special_tokens=True)[0]
            else:
                raise RuntimeError("Tokenizeræœªæ­£ç¡®åŠ è½½")
            
            # æ¸…ç†è½¬å½•ç»“æœ
            transcription = self._clean_transcription(transcription)
            
            return transcription
            
        except Exception as e:
            logger.error(f"æ¨¡å‹è½¬å½•å¤±è´¥: {e}")
            # è¿”å›æ¼”ç¤ºæ–‡æœ¬
            return "This is a demonstration transcription. In production, this should be replaced with actual audio transcription."
    
    def _apply_chat_template(self, messages) -> str:
        """åº”ç”¨èŠå¤©æ¨¡æ¿"""
        formatted_text = ""
        for message in messages:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                formatted_text += f"```system\n{content}\n```\n"
            elif role == "user":
                formatted_text += f"```user\n{content}\n```\n"
            elif role == "assistant":
                formatted_text += f"```assistant\n{content}\n```\n"
        
        formatted_text += "```assistant\n\n```"
        return formatted_text
    
    def _clean_transcription(self, text: str) -> str:
        """æ¸…ç†è½¬å½•æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºæ ¼
        text = text.strip()
        
        # ç¡®ä¿å¥å­ä»¥é€‚å½“çš„æ ‡ç‚¹ç»“å°¾
        if text and not text[-1] in '.!?':
            text += '.'
        
        return text

class SentenceSegmenter:
    """è‹±æ–‡åˆ†å¥å™¨"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–åˆ†å¥å™¨
        
        Args:
            config: åˆ†å¥é…ç½®
        """
        self.config = config or {}
        self.strong_boundaries = self.config.get('patterns', {}).get('strong_boundaries', ['.', '!', '?', ';'])
        self.weak_boundaries = self.config.get('patterns', {}).get('weak_boundaries', [',', 'and', 'but', 'or', 'so'])
        self.clause_markers = self.config.get('patterns', {}).get('clause_markers', ['that', 'which', 'who', 'when', 'where'])
    
    def segment_sentences(self, text: str) -> List[Dict[str, Any]]:
        """
        å°†æ–‡æœ¬åˆ†å¥
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            List[Dict[str, Any]]: åˆ†å¥ç»“æœ
        """
        try:
            # åŸºç¡€åˆ†å¥ï¼ˆæŒ‰å¼ºè¾¹ç•Œåˆ†å‰²ï¼‰
            sentences = self._split_by_strong_boundaries(text)
            
            # è¿›ä¸€æ­¥å¤„ç†é•¿å¥
            processed_sentences = []
            for sentence in sentences:
                if self._is_sentence_too_long(sentence):
                    sub_sentences = self._split_long_sentence(sentence)
                    processed_sentences.extend(sub_sentences)
                else:
                    processed_sentences.append(sentence)
            
            # æ„å»ºç»“æœ
            result = []
            for i, sentence in enumerate(processed_sentences):
                sentence = sentence.strip()
                if sentence:  # è·³è¿‡ç©ºå¥å­
                    word_count = len(sentence.split())
                    result.append({
                        'index': i + 1,
                        'text': sentence,
                        'word_count': word_count,
                        'character_count': len(sentence),
                        'complexity': self._assess_complexity(sentence)
                    })
            
            logger.info(f"âœ… åˆ†å¥å®Œæˆï¼Œå…± {len(result)} ä¸ªå¥å­")
            return result
            
        except Exception as e:
            logger.error(f"âŒ åˆ†å¥å¤±è´¥: {e}")
            # å³ä½¿å‡ºé”™ä¹Ÿè¦è¿”å›ä¸€ä¸ªåˆç†çš„é»˜è®¤å€¼
            return [{
                'index': 1,
                'text': text.strip() if text else '',
                'word_count': len(text.split()) if text else 0,
                'character_count': len(text) if text else 0,
                'complexity': 'unknown'
            }]
    
    def _split_by_strong_boundaries(self, text: str) -> List[str]:
        """æŒ‰å¼ºè¾¹ç•Œåˆ†å‰²"""
        if not text:
            return []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²ï¼Œä½†ä¿ç•™åˆ†éš”ç¬¦
        import re
        pattern = r'([.!?;]+)'
        parts = re.split(pattern, text)
        
        # é‡æ–°ç»„åˆå¥å­å’Œæ ‡ç‚¹ç¬¦å·
        sentences = []
        current_sentence = ""
        
        for part in parts:
            if re.match(r'[.!?;]+', part):
                # è¿™æ˜¯æ ‡ç‚¹ç¬¦å·ï¼Œæ·»åŠ åˆ°å½“å‰å¥å­
                current_sentence += part
                sentences.append(current_sentence.strip())
                current_sentence = ""
            else:
                # è¿™æ˜¯æ–‡æœ¬éƒ¨åˆ†
                current_sentence += part
        
        # æ·»åŠ æœ€åä¸€ä¸ªå¥å­ï¼ˆå¦‚æœæ²¡æœ‰ä»¥æ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼‰
        if current_sentence.strip():
            sentences.append(current_sentence.strip())
        
        # æ¸…ç†ç©ºå¥å­
        sentences = [s.strip() for s in sentences if s.strip()]
        
        return sentences
    
    def _is_sentence_too_long(self, sentence: str) -> bool:
        """åˆ¤æ–­å¥å­æ˜¯å¦è¿‡é•¿"""
        word_count = len(sentence.split())
        max_length = self.config.get('rules', {}).get('max_sentence_length', 30)
        return word_count > max_length
    
    def _split_long_sentence(self, sentence: str) -> List[str]:
        """åˆ†å‰²é•¿å¥"""
        # å°è¯•æŒ‰é€—å·åˆ†å‰²
        parts = sentence.split(',')
        
        if len(parts) > 1:
            # é‡æ–°ç»„åˆï¼Œç¡®ä¿æ¯éƒ¨åˆ†ä¸ä¼šå¤ªçŸ­
            result = []
            current_part = ""
            
            for part in parts:
                part = part.strip()
                if len(current_part.split()) < 5:  # å¦‚æœå½“å‰éƒ¨åˆ†å¤ªçŸ­ï¼Œç»§ç»­æ·»åŠ 
                    current_part += (", " if current_part else "") + part
                else:
                    result.append(current_part)
                    current_part = part
            
            if current_part:
                result.append(current_part)
            
            return result
        
        # å¦‚æœæ— æ³•æŒ‰é€—å·åˆ†å‰²ï¼Œè¿”å›åŸå¥
        return [sentence]
    
    def _assess_complexity(self, sentence: str) -> str:
        """è¯„ä¼°å¥å­å¤æ‚åº¦"""
        word_count = len(sentence.split())
        
        # æ£€æŸ¥å¤æ‚ç»“æ„
        has_subordinate_clause = any(marker in sentence.lower() for marker in self.clause_markers)
        has_multiple_clauses = sentence.count(',') > 1
        
        if word_count > 20 or has_subordinate_clause or has_multiple_clauses:
            return 'complex'
        elif word_count > 10:
            return 'medium'
        else:
            return 'simple'

def test_audio_transcriber():
    """æµ‹è¯•éŸ³é¢‘è½¬å½•å™¨"""
    try:
        # åˆ›å»ºæµ‹è¯•éŸ³é¢‘æ–‡ä»¶
        import tempfile
        
        # ç”Ÿæˆç®€å•çš„æµ‹è¯•éŸ³é¢‘
        sample_rate = 16000
        duration = 2
        frequency = 440
        
        t = np.linspace(0, duration, int(sample_rate * duration))
        audio_data = 0.5 * np.sin(2 * np.pi * frequency * t)
        
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
            sf.write(tmp_file.name, audio_data, sample_rate)
            
            # æµ‹è¯•è½¬å½•
            transcriber = AudioTranscriber()
            result = transcriber.transcribe_audio(tmp_file.name)
            
            print(f"âœ… è½¬å½•ç»“æœ: {result}")
            
            # æµ‹è¯•åˆ†å¥
            segmenter = SentenceSegmenter()
            sentences = segmenter.segment_sentences(result.get('transcription', 'Hello world. This is a test.'))
            
            print(f"âœ… åˆ†å¥ç»“æœ: {sentences}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            import os
            os.unlink(tmp_file.name)
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    test_audio_transcriber()
