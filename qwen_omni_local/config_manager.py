#!/usr/bin/env python3
"""
é…ç½®ç®¡ç†å™¨ - å¤„ç†YAMLé…ç½®æ–‡ä»¶
"""

import yaml
import os
from typing import Dict, Any, List
import logging

logger = logging.getLogger(__name__)

class ConfigManager:
    """é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self, config_path: str = "prompts.yml"):
        """
        åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        
        Args:
            config_path: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = config_path
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        try:
            if not os.path.exists(self.config_path):
                raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
            
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            logger.info(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            return config
            
        except Exception as e:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def get_system_prompt(self) -> str:
        """è·å–ç³»ç»Ÿæç¤ºè¯"""
        return self.config.get('system_prompt', '')
    
    def get_evaluation_prompt(self, prompt_type: str = 'detailed_evaluation') -> Dict[str, str]:
        """
        è·å–è¯„ä¼°æç¤ºè¯
        
        Args:
            prompt_type: æç¤ºè¯ç±»å‹
            
        Returns:
            Dict[str, str]: åŒ…å«name, description, promptçš„å­—å…¸
        """
        evaluation_prompts = self.config.get('evaluation_prompts', {})
        
        if prompt_type not in evaluation_prompts:
            available_types = list(evaluation_prompts.keys())
            raise ValueError(f"æœªçŸ¥çš„æç¤ºè¯ç±»å‹: {prompt_type}. å¯ç”¨ç±»å‹: {available_types}")
        
        return evaluation_prompts[prompt_type]
    
    def list_available_prompts(self) -> List[Dict[str, str]]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æç¤ºè¯"""
        evaluation_prompts = self.config.get('evaluation_prompts', {})
        result = []
        
        for key, value in evaluation_prompts.items():
            result.append({
                'type': key,
                'name': value.get('name', key),
                'description': value.get('description', 'æ— æè¿°')
            })
        
        return result
    
    def get_output_format(self) -> Dict[str, Any]:
        """è·å–è¾“å‡ºæ ¼å¼é…ç½®"""
        return self.config.get('output_format', {})
    
    def get_sentence_segmentation_config(self) -> Dict[str, Any]:
        """è·å–åˆ†å¥é…ç½®"""
        return self.config.get('sentence_segmentation', {})
    
    def get_evaluation_config(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°é…ç½®"""
        return self.config.get('evaluation_config', {})
    
    def build_full_prompt(self, prompt_type: str, transcription: str, sentence_analysis: List[Dict] = None) -> str:
        """
        æ„å»ºå®Œæ•´çš„è¯„ä¼°æç¤ºè¯
        
        Args:
            prompt_type: æç¤ºè¯ç±»å‹
            transcription: è½¬å½•æ–‡æœ¬
            sentence_analysis: åˆ†å¥åˆ†æç»“æœ
            
        Returns:
            str: å®Œæ•´çš„æç¤ºè¯
        """
        system_prompt = self.get_system_prompt()
        evaluation_prompt_data = self.get_evaluation_prompt(prompt_type)
        evaluation_prompt = evaluation_prompt_data['prompt']
        
        # æ„å»ºåˆ†å¥åˆ†æéƒ¨åˆ†
        sentence_info = ""
        if sentence_analysis:
            sentence_info = "\n**åˆ†å¥åˆ†æç»“æœï¼š**\n"
            for i, sentence_data in enumerate(sentence_analysis, 1):
                sentence_info += f"{i}. \"{sentence_data['text']}\" (é•¿åº¦: {sentence_data['word_count']}è¯)\n"
        
        # æ„å»ºå®Œæ•´æç¤ºè¯
        full_prompt = f"""{system_prompt}

{evaluation_prompt}

**éœ€è¦è¯„ä¼°çš„è‹±è¯­å£è¯­å†…å®¹ï¼š**
åŸæ–‡: "{transcription}"
{sentence_info}

**è¾“å‡ºè¦æ±‚ï¼š**
è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- overall_score: æ€»ä½“è¯„åˆ† (0-100)
- transcription: å®Œæ•´è½¬å½•æ–‡æœ¬
- sentence_analysis: åˆ†å¥åˆ†æç»“æœ
- evaluation: å„ç»´åº¦è¯¦ç»†è¯„åˆ†å’Œåé¦ˆ
- detailed_analysis: æ·±å…¥åˆ†æï¼ˆä¼˜åŠ¿ã€é—®é¢˜ã€é”™è¯¯ç¤ºä¾‹ï¼‰
- suggestions: å…·ä½“æ”¹è¿›å»ºè®®
- metadata: å…ƒæ•°æ®ä¿¡æ¯

è¯·ç¡®ä¿è¯„ä¼°å…·ä½“ã€è¯¦ç»†ã€å®ç”¨ï¼Œæä¾›å¯æ“ä½œçš„æ”¹è¿›å»ºè®®ã€‚"""

        return full_prompt
    
    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®æ–‡ä»¶å®Œæ•´æ€§"""
        required_sections = [
            'system_prompt',
            'evaluation_prompts',
            'output_format',
            'sentence_segmentation',
            'evaluation_config'
        ]
        
        missing_sections = []
        for section in required_sections:
            if section not in self.config:
                missing_sections.append(section)
        
        if missing_sections:
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…è¦éƒ¨åˆ†: {missing_sections}")
            return False
        
        # éªŒè¯è¯„ä¼°æç¤ºè¯
        evaluation_prompts = self.config.get('evaluation_prompts', {})
        if not evaluation_prompts:
            logger.error("âŒ æ²¡æœ‰é…ç½®è¯„ä¼°æç¤ºè¯")
            return False
        
        for prompt_type, prompt_data in evaluation_prompts.items():
            if 'prompt' not in prompt_data:
                logger.error(f"âŒ æç¤ºè¯ {prompt_type} ç¼ºå°‘ prompt å­—æ®µ")
                return False
        
        logger.info("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")
        return True
    
    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®æ–‡ä»¶"""
        self.config = self._load_config()
        logger.info("ğŸ”„ é…ç½®æ–‡ä»¶å·²é‡æ–°åŠ è½½")
    
    def get_scoring_weights(self) -> Dict[str, float]:
        """è·å–è¯„åˆ†æƒé‡"""
        eval_config = self.get_evaluation_config()
        return eval_config.get('scoring', {}).get('weights', {
            'pronunciation': 0.25,
            'fluency': 0.25,
            'grammar': 0.20,
            'vocabulary': 0.20,
            'content': 0.10
        })
    
    def get_grade_boundaries(self) -> Dict[str, List[int]]:
        """è·å–è¯„åˆ†ç­‰çº§è¾¹ç•Œ"""
        eval_config = self.get_evaluation_config()
        return eval_config.get('scoring', {}).get('grade_boundaries', {
            'excellent': [90, 100],
            'good': [80, 89],
            'fair': [70, 79],
            'pass': [60, 69],
            'fail': [0, 59]
        })
    
    def get_grade_for_score(self, score: int) -> str:
        """æ ¹æ®åˆ†æ•°è·å–ç­‰çº§"""
        boundaries = self.get_grade_boundaries()
        
        for grade, (min_score, max_score) in boundaries.items():
            if min_score <= score <= max_score:
                return grade
        
        return 'unknown'

def test_config_manager():
    """æµ‹è¯•é…ç½®ç®¡ç†å™¨"""
    try:
        config_manager = ConfigManager()
        
        # éªŒè¯é…ç½®
        if not config_manager.validate_config():
            print("âŒ é…ç½®éªŒè¯å¤±è´¥")
            return False
        
        # æµ‹è¯•è·å–æç¤ºè¯
        prompts = config_manager.list_available_prompts()
        print(f"âœ… å¯ç”¨æç¤ºè¯æ•°é‡: {len(prompts)}")
        for prompt in prompts:
            print(f"  - {prompt['type']}: {prompt['name']}")
        
        # æµ‹è¯•æ„å»ºå®Œæ•´æç¤ºè¯
        test_transcription = "Hello, my name is John. I am very excited to be here today."
        full_prompt = config_manager.build_full_prompt('detailed_evaluation', test_transcription)
        print(f"âœ… å®Œæ•´æç¤ºè¯é•¿åº¦: {len(full_prompt)} å­—ç¬¦")
        
        # æµ‹è¯•è¯„åˆ†ç­‰çº§
        test_scores = [95, 85, 75, 65, 55]
        for score in test_scores:
            grade = config_manager.get_grade_for_score(score)
            print(f"  åˆ†æ•° {score} -> ç­‰çº§ {grade}")
        
        print("âœ… é…ç½®ç®¡ç†å™¨æµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    test_config_manager()
