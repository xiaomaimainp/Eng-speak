#!/usr/bin/env python3
"""
å¿«é€Ÿæµ‹è¯•Qwen2.5 Omniæ¨¡å‹çš„ç®€åŒ–ç‰ˆæœ¬
"""
import torch
from transformers import AutoTokenizer
import sys

# å¯¼å…¥Qwen2.5 Omniæ¨¡å‹
try:
    from transformers import Qwen2_5OmniForConditionalGeneration  # type: ignore
    QWEN_OMNI_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥Qwen2_5OmniForConditionalGeneration")
except ImportError as e:
    QWEN_OMNI_AVAILABLE = False
    print(f"âŒ Qwen2_5OmniForConditionalGenerationä¸å¯ç”¨: {e}")
    sys.exit(1)

# æ¨¡å‹è·¯å¾„
MODEL_PATH = "/data/qwen_omni/"

def quick_test():
    """å¿«é€Ÿæµ‹è¯•æ¨¡å‹åŠŸèƒ½"""
    print("ğŸš€ å¼€å§‹å¿«é€Ÿæµ‹è¯•...")
    
    # åŠ è½½tokenizer
    print("ğŸ“ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    
    # è®¾ç½®pad_token_id
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # åŠ è½½æ¨¡å‹ï¼ˆä»…CPUï¼Œé¿å…GPUå†…å­˜é—®é¢˜ï¼‰
    print("ğŸ¤– åŠ è½½æ¨¡å‹åˆ°CPU...")
    model = Qwen2_5OmniForConditionalGeneration.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float32,
        device_map="cpu",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    
    # ç¦ç”¨éŸ³é¢‘åŠŸèƒ½
    if hasattr(model.config, 'enable_audio_output'):
        model.config.enable_audio_output = False
        print("ğŸ”‡ å·²ç¦ç”¨éŸ³é¢‘è¾“å‡º")
    
    if hasattr(model.config, 'enable_talker'):
        model.config.enable_talker = False
        print("ğŸ”‡ å·²ç¦ç”¨talkeråŠŸèƒ½")
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    # å‡†å¤‡è¾“å…¥
    text_input = "Hello, how are you?"
    if len(sys.argv) > 1:
        text_input = " ".join(sys.argv[1:])
    
    print(f"ğŸ’¬ è¾“å…¥: {text_input}")
    
    # ç®€å•çš„æ–‡æœ¬æ ¼å¼åŒ–
    formatted_text = f"<|im_start|>user\n{text_input}<|im_end|>\n<|im_start|>assistant\n"
    
    # ç¼–ç 
    print("ğŸ”¤ ç¼–ç è¾“å…¥...")
    inputs = tokenizer(formatted_text, return_tensors="pt")
    
    # ç”Ÿæˆï¼ˆä½¿ç”¨æœ€ç®€å•çš„å‚æ•°ï¼‰
    print("âš¡ å¼€å§‹ç”Ÿæˆï¼ˆä½¿ç”¨æœ€ç®€å‚æ•°ï¼‰...")
    try:
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=50,  # å¾ˆçŸ­çš„è¾“å‡º
                do_sample=False,    # è´ªå©ªè§£ç 
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç è¾“å‡º
        generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
        
        print("ğŸ‰ ç”ŸæˆæˆåŠŸ!")
        print(f"ğŸ¤– æ¨¡å‹å›å¤: {generated_text}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    quick_test()
